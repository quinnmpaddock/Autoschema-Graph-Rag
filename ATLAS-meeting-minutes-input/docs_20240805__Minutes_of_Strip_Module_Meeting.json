{
    "id": "192",
    "text": "# 2024-08-05 - Minutes of Strip Module Meeting\nhttps://indico.cern.ch/event/1442746/\n### Introduction\nPresenter: Luise Poley\n- Luise: [Asked a question of a bullet point on slide 2, I missed the question]\n - Anne: We are occupied with the interposer modules for now\n- Peter Phillips (sl. 4): I have a correction. We are testing three staves at once in different systems, but not one is half and half SE4445 and Hysol. [Peter enumerates several different staves]\n - Gabriele: Just to clarify, the RAL and BNL points are referring to the same stave.\n - Luise: We will update this.\n- Bruce: WHere does this come from? I thought in previous meetings we wouldn\u2019t change the alternative identifier. I suggest it does not change. If we are going to show this slide, it should be a question that requires a lot of agreement to change. If so, let the DB people know. Sorry. The alternative identifier shouldn\u2019t change\n - Luise: The problem is if it\u2019s not being used for anything we could change it. We will follow up offline\n- Sven: On a side note, if you have or observe any DB issues, please let me know. It would be good to investigate why you think there are problems with the alternative identifiers.\n - Luise: Okay, we\u2019ll start a conversation about this.\n### QT Progress Update: HV Stability\nPresenter: Judith Kull\n- Vitaliy: You mentioned ten hours, I didn\u2019t understand that. The Sensor groups use 40-hour long tests\n - Judith: must have misunderstood\n - Vitaliy: not a big deal, just for completeness\n- Vitaliy: The slide with the current trends. It went fast. Which ones are failing in this plot?\n - Judith: the ones at the top are failing, may also be included in the plots below\n - Vitaliy: The ones on top are violating the criteria?\n - Judith: If using IVar, then yes. The ones below could fail if you used either high std or high average current.\n - Vitaliy: If I look at the top plot, the lines are pretty stable, so I\u2019m surprised IVar is failing. I\u2019m wondering if there is a numerical issue. The middle plot has weird spikes, so that\u2019s a bit hard to understand as well (bottom too). That I haven\u2019t seen in the sensor QC data, and I\u2019ve looked at all of them. Upward spikes are worrying, downward spikes are \u2026 weird. The plots also mirror each other a bit, making me wonder if the tests were performed at the same time. Some readout issue\n - Judith: Some of these are the same test\n- Vitaliy: some of the graphs show a current trending down, which could be flagged as high deviation, but they are usually passed because it\u2019s just a trend, not a fluctuation\n - Judith: \n- Vitaliy: I don\u2019t understand the high leakage current comment. 2 uA isn\u2019t very high. It\u2019s higher than it typically is, but otherwise fine.\n - Judith: mainly means it\u2019s higher than typical, but could also mask a module that has a high fluctuation. The concern is not a high current but a high fluctuation\n - Vitaliy: If std [?], I think higher current is an orthogonal variable. \n- Luise: have you plotted these together with just the sensor current?\n - Judith: not yet, but I did start looking into it. Not all the modules necessarily have a sensor-stability test\n - Luise: understood, if you find one, that would be really interesting to compare!\n- Luise: It may be good to start by establishing what leakage current criteria should be. You could have a slope of the curve that could be interpreted as changes but is actually more of a trend. Currently, we are just taking the data and don\u2019t have a pass/fail\n- William: I missed something, I thought these were module IV stability. \n - Judith: No, these are module stability tests\n - William: and Luise suggests comparing them to the sensor stability?\n - Judith: Yes, so potentially might have a look at it.\n - William: we are struggling to understand where the 11 R3s come from, we did fewer than that, but maybe DESY also did some\n### L4 Updates\n- Read Hannah\u2019s Minutes\n### Data Merger Status\nPresenter: Cole Helling\n- Luise (slide 6) this was a qualification task by Erik Wallin: to look into whether in addition to passing and failing we also need to check whether there are trends indicating that things are getting worse\n - Cole: reached out to Erik already; may not need a new qualification task, may be a qualification task that was already done\n- Luise (slide 8): can there be a data visualization step before uploading the results? I.e. test finishes, then the user sees the data before it is uploaded?\n - Cole: need to discuss with Peter and Bruce, but it is a good idea\n- Bruce: this is work in progress, need to discuss a lot of the details, people should have a look at the merge request and comment and there is a lot of stuff in this merge request, not all of this should go in\n- Bruce: not sure there should be an option to review, rather should make sure that people can rely on the functionality\n - Luise: we do see cases of people uploading data without checking it, so it would help if people got a very quick visual confirmation before uploading it\n - Peter: need to make sure that ITSDAQ catches as many problems as possible automatically\n - Cole: would prefer both: automatic checks in ITSDAQ and visualization for people to look at\n- No objections\n### BNL Uploader Moved to Next Week\nPresenter: Gabriele D\u2019Amen\n### Agreeing on an Interposer Assembly Procedure Moved to Next Week\n### Freiburg Sensor Recovery Experience\nPresenter: Roland\n- Vitaliy: The current scale, it sounds like a normalization difference. The curves are different by a factor of 100 in some cases, so it might not actually normalized\n - Roland: could be, may need to check\n - Vitaliy: Sounds good\n- Vitaliy: Jumpy IVs. I\u2019m kind of suspicious it\u2019s a setup issue. I think it was discussed at this meeting at some point. It depends on how your DAQ works. It needs to allow enough settling time for the PS. It\u2019s present for a few plots.\n - Roland: we have the settling time in our setup, 5-10 seconds? And then only use the last one as the data point. Could be a setup effect, but it looks fine for most of these. We can investigate. Probably we can take one of the spiky IV sensors and repeat the IV\n - Vitaliy: You can repeat IVs, or have an artificially large resistor to mock this up. It reminds me, when you did initial testing, was it one IV per sensor? Did you re-run any after deionization?\n - Roland: James says yes\n - Vitaliy: It\u2019s just that some of the sensors could benefit from repeat IVs. It may not always even need the ion blower.\n - Roland: [misunderstanding]\n - Vitaliy: You did a fairly careful analysis here. It would have been simpler after running the initial IVs, to simply re-run the IV immediately. The second can often be better than the first one.\n - Roland: will check in the database, we uploaded all of them, should be in the database for some of them\n- Vitaliy again: You asked about the origin of the static charge. The packaging in the ESD sheets and envelopes is not as ESD-safe as one would hope. So shipment can lead to these effects. But handling is also important, so it\u2019s good to run a blower.\n - Roland: okay\n- Somehow still Vitaliy: About if the charge is gone, yeah it\u2019s gone. What gets recovered stays recovered. There were studies of the effects of very small doses of irradiation (ionizing). A few day\u2019s worth in the detector is considered enough, so I don\u2019t think we need to worry about this.\n - Roland: It seems like the followup question is \u201cHow long should we do it for?\u201d. Should we do this for two hours at reception? Looking at the yields it was 95% at 30 minutes, and I think the goal is to make sure we have less than 3% with problems. Is this reasonable, is this wanted by the community?\n - Xavi: Yes, two hours is too much. What we recommend is to use 10-20 minutes, not much more. I\u2019m surprised you could still recover them after two hours. \n- Xavi: different ion blowers require different times, your seems to require about five minutes to completely deionise something (based on data from sensor community), would recommend it for about 30 seconds before repeating IVs, and maybe repeating IVs in general. Two hours is too much\n- Xavi: in some plots, you measure more than 10 uA, up to 20 uA, should reduce compliance to 10 uA\n - Roland: It could be the historical compliance, we\u2019ll check\n- Xavi: you mention you have some IVs with the wrong units in the database; if this is a small number, would be good to reupload, if it\u2019s not too much work\n - Roland: Should I also delete the wrong ones?\n - Xavi: yes please\n- I\u2019m gonna put a rule in that one hand, one question. \n- Luise: For the talk Alvaro is about to give, regarding how many failures \u2026 wait, it\u2019ll be interesting. It\u2019ll hopefully show the rate will go down in the future.\n- Luise: For how much we can live with (sl. 13): we need to look into this as well on the module side. For that, we really need you to upload recovery tests to the database. If you recover a sensor, it needs to be added to the database and you say how long you ran the ion blower so that we can later reconstruct the data. I just checked and I don\u2019t see any uploaded yet.\n - Roland: I was not aware, I will check and upload. Thanks a lot.\n - Luise: So yes, we are worried and plan to look into this further with another QT\n- Xavi again: Do we need to still leave the sensors in dry storage for several weeks\n - Luise: There is only a requirement to store them dry and not how long to store them dry.\n - Xavi: I\u2019m not sure we completely benefit from long periods\n - Roland: We had to measure other things first, so it took a long time. It was just in our case.\n - Luise: The original attempt was if it fails, wait a week of it sitting dry. \n### Is Your Visual Inspection Hiding a Shocking Secret? Discover the Truth Behind the Results from More Than 200 Sensors Visually Inspected at DESY\nPresenter: Alvaro Lopez Solis\n- Xavi: In slide 5, you mentioned that there is a pattern. I can confirm that. The R5 sensors were tested at TRIUMF. I\u2019m preparing a presentation to talk about this soon, but we found during pre-production testing that the metal pins can induce the chips. So we stopped using those jigs. That\u2019s why you see those chips. How many cases from the R5 batch?\n - Alvaro: I have around 7 or 8. We still have more to test.\n - Xavi: Okay, it would be good to know the total number. I was thinking that if we see too many from the same batch, and the chips are less than 50um for example, maybe we can talk about making an exception. It\u2019s something we can discuss when we have the total number. \n - Alvaro: yes, we should take the IVs for these\n - Xavi: Yes. It would be interesting to see how the IV looks. The problems with these chips is that maybe the IV is okay, but it might not be alright with the stability.\n- Xavi again: On slide (Doubts if failing or not), it\u2019s true that the strips are not connected, but you can see the metal is damaged, so the passivation is also likely damaged. This is an example of the limit between light and heavy scratches. This should fail. \n - Alvaro: okay, will do, thanks\n- Vitaliy: 9000 points. In the cases you see scratches, do you also see chips around the corners or the edges.\n - Alvaro: some of them, yes. Didn\u2019t fail this one, because it wasn\u2019t always clear\n - Vitaliy: That\u2019s what we saw in the sensor community. A chip somewhere in the corner can \u201ctravel\u201d and that\u2019s the chip that causes scratches.\n- Vitaliy: it\u2019s very nice you tracked locations down to test frames and checked the diode locations, \n - Luise: Dennis, the LEDs for EC are plastic right?\n - Dennis: only epoxy touching the sensor\n - Vitaliy: That\u2019s good to know, maybe it\u2019s not the test frame placement\n - Dennis: If the sensor sits on top, there is enough leverage to cause problems. But if it\u2019s next to it, hard to say\n - Vitaliy: It might be useful for someone to play with half-moons.\n- Vitaliy: wouldn\u2019t expect IVs to show problems for chipped sensors [sharing screen to show why it shouldn\u2019t be a problem: edge is expected to be on high voltage, central area is expected to be on ground, high gradient is expected over guard ring, so chips around the edge shouldn\u2019t actually cause early breakdown\n- \n- Luise: Just to make sure we agree. Alvaro, you will complete the summary and Xavi, we will do a dedicated test to see how useful the sensors with chips are. Is this correct?\n - Alvaro: Yes, I will get the rest of the sensors.\n - Luise: Next step is to do the IVs to convince ourselves we don\u2019t have a problem and then we can run a stability test to see that there is no correlation with chips and perhaps relax the constraints on chip size. Everyone happy with that?\n - Xavi: We have examples of these chips at TRIUMF. I\u2019m wondering if Vitaliy, do you think if during module assembly, these chips can cause issues with cracking?\n - Vitaliy: I\u2019d be concerned that the chip is just what we see, but in some cases it could be the tip of the iceberg and hide a crack that will develop later. If we want to build test modules, we could try, if we want to use them later in the detector, we should be careful\n - Xavi: I think we can resume this conversation when we have the full overview from Alvaro. \n - Vitaliy: cracks developing later is a concern, but we haven\u2019t actually seen it, so worth investigating if it can happen\n### Category A and Category B Hybrid Comparison\nPresenter: Marcus Wong\n- Peter Phillips: I want to point out that we\u2019ve been on similar territory before. There is a difference between dead channels and untrimmable channels. It seems like you have the latter. Why? I\u2019m not sure. One possible contribution is that if you go from a batch of chips tested at DAI vs at SCIPP, DAI might have left one [?]. Any channel that has a bad trimmed value could be switched off. The temperature could also play a role, as they would be a lot hotter during burn-in than during ASIC probing. The gain is often a little strange on the ends. We know the gain goes down when it goes colder, so it should go up when it goes up. Interposed hybrids will be hotter because of the extra layer, so it\u2019s quite possible this is a thermal effect and it will be interesting to see the same results with better cooling\n- William: That\u2019s a decent explanation. I was also worried it would be hotter. But you aren\u2019t worried they aren\u2019t trimmable? \n- William: do you have category A hybrids that have been interposed\n - Marcus: No, I don\u2019t think we have any plans to interpose CatA right now\n - Vitaliy: cat A is too precious to use on the first round\n - William: But it seems like we need to do this\n - Luise: There is a plan to use the production hybrids that were Cat A, so we should have some available in the not too distant future.\n- Luise: For the plots you\u2019re showing, could you go through and see how many had known bad channels so we can take out the \u201cold\u201d bad channels? On your middle plot of slide (channel failure: relative chip location), take out the already-known bad channels.\n - Marcus: Repeats \n - Luise: Yes. See how many new bad channels there are.\n- Luise: Could you look into what Peter just said and see where your wafers were probed? I think there is a testing location in the DB. Is that correct?\n - Peter: Yes.\n - Luise: So it would be good to check if the middle plot is mostly made from DA ASICs and see if they weren\u2019t flagged. Make sense?\n - Marcus: Yes\n- Vitaliy: For the channels which are untrimmable, is there an easy way to tell?\n - Peter: if there is a value of -1\n - Vitaliy: So -1 means untrimmable, and other numbers mean other things?\n - Peter: The trim values are between 0 and 31, but dead channels are also \u201cnot trimmable\u201d\n - Vitaliy: Right, but how do we distinguish them? \n - Peter: will have a think about this, will have to have a look. May already be in the json files\n - Vitaliy: If you would, that would be useful so we can tel the results better.\n- Vitaliy: Does it make sense to change any values and retry the tests?\n - Peter: not sure. Was wondering whether these chips were originally tested at 1.2 V digital voltage and if something was done to increase that to 1.25 V. Should bear in mind that the pedestal trim is using a default target determined by testing chips at 20 degrees. These may be trimmable again if we adjust the target slightly, even at elevated temperatures, in case we need to use them in the detector\n - Cole: Would it make sense to run the full pedestal trim scan to see if those values make sense?\n - Peter: You can also try it manually, but I suppose you could.\n - Vitaliy: happy to do that, just tell us what to do\n- Jaya John: Marcus, the wafers that were tested at DAI will be marked as Carlton in the DB.\n - Peter: if you could just ping me on mattermost and tell me wafer names, then I\u2019d like to have a look at the data myself, and it would be nice to look whether these are wafers that were just worse than others\n - Marcus: Okay\n### AOB\n- Office hour!",
    "metadata": {
        "lang": "en"
    }
}